{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"TechLabs.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Please enter your full name here: Amazir Nimgharen</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p> This exam will test your knowledge in Artificial Intelligence. </p>\n",
    "<p> We will test the following: </p>\n",
    "\n",
    "- Logistic Regressions\n",
    "- Neural Networks\n",
    "- Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1:\n",
    "Below is the code to import a standard dataset with breast cancer.\n",
    "When you run the cell, you will have it stored as the object \"data\",\n",
    "and the description of the dataset is printed out for you.\n",
    "\n",
    "### Your task is to create two models for classifying the diagnosis, and compare the accuracy metrics for the two:\n",
    "### 1. a) Logistic Regression: \n",
    "\"sklearn.linear_model\" offers good logit classifiers. Because the dataset is fairly small (~500 rows), we suggest that you use the \"liblinear\" solver for fitting the logit classifier.\n",
    "\n",
    "Split the dataset into a test and training (30% test / 70% training) dataset and train the model on the training dataset. Then, classify the test dataset, and compute the accuracy, which you can print as an output.\n",
    "\n",
    "P.S.: We do not expect you to regularize for type 1 or type 2 errors. Your goal is to build models that classify well the diagnosis. Also, we do not expect you to spend hours on feature engineering and/or other things that may or may not improve testing accuracy. A good answer shall just contain a sensible model and the testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
    "print(np.shape(X.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.49%\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver=\"liblinear\").fit(X_train, y_train)\n",
    "score = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Accuracy: {score.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the logit accuracy below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing set accuracy with logit is: 96.49%\n"
     ]
    }
   ],
   "source": [
    "logitacc = \"96.49%\"\n",
    "print(f\"\\ntesting set accuracy with logit is: {logitacc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. b) Feedforward Neural Network:\n",
    "So far, so good. Let's see how a feedforward neural network does for the same task.\n",
    "\n",
    "Take the same train/test split as in a), and construct the following neural network, which you fit to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Model: \"sequential_2\"',\n",
       " '_________________________________________________________________',\n",
       " 'Layer (type)                 Output Shape              Param #   ',\n",
       " '=================================================================',\n",
       " 'dense_5 (Dense)              (None, 30)                930       ',\n",
       " '_________________________________________________________________',\n",
       " 'dense_6 (Dense)              (None, 50)                1550      ',\n",
       " '_________________________________________________________________',\n",
       " 'dense_7 (Dense)              (None, 30)                1530      ',\n",
       " '_________________________________________________________________',\n",
       " 'dense_8 (Dense)              (None, 1)                 31        ',\n",
       " '=================================================================',\n",
       " 'Total params: 4,041',\n",
       " 'Trainable params: 4,041',\n",
       " 'Non-trainable params: 0',\n",
       " '_________________________________________________________________']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.loads(b'\\x80\\x03]q\\x00(X\\x15\\x00\\x00\\x00Model: \"sequential_2\"q\\x01XA\\x00\\x00\\x00_________________________________________________________________q\\x02XA\\x00\\x00\\x00Layer (type)                 Output Shape              Param #   q\\x03XA\\x00\\x00\\x00=================================================================q\\x04XA\\x00\\x00\\x00dense_5 (Dense)              (None, 30)                930       q\\x05XA\\x00\\x00\\x00_________________________________________________________________q\\x06XA\\x00\\x00\\x00dense_6 (Dense)              (None, 50)                1550      q\\x07XA\\x00\\x00\\x00_________________________________________________________________q\\x08XA\\x00\\x00\\x00dense_7 (Dense)              (None, 30)                1530      q\\tXA\\x00\\x00\\x00_________________________________________________________________q\\nXA\\x00\\x00\\x00dense_8 (Dense)              (None, 1)                 31        q\\x0bXA\\x00\\x00\\x00=================================================================q\\x0cX\\x13\\x00\\x00\\x00Total params: 4,041q\\rX\\x17\\x00\\x00\\x00Trainable params: 4,041q\\x0eX\\x17\\x00\\x00\\x00Non-trainable params: 0q\\x0fXA\\x00\\x00\\x00_________________________________________________________________q\\x10e.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can use the library keras to contruct the ANN. Since we are dealing with a classification problem, we recommend binary_crossentropy as a loss function.\n",
    "Choose a reasonable batch size and epoch count and train the model.\n",
    "### Here again, print the testing dataset accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 398 samples, validate on 171 samples\n",
      "Epoch 1/100\n",
      "398/398 [==============================] - 0s 589us/step - loss: 10.5554 - accuracy: 0.5327 - val_loss: 7.4096 - val_accuracy: 0.6316\n",
      "Epoch 2/100\n",
      "398/398 [==============================] - 0s 40us/step - loss: 8.2561 - accuracy: 0.4799 - val_loss: 0.7605 - val_accuracy: 0.8187\n",
      "Epoch 3/100\n",
      "398/398 [==============================] - 0s 43us/step - loss: 3.3901 - accuracy: 0.6030 - val_loss: 0.9791 - val_accuracy: 0.8129\n",
      "Epoch 4/100\n",
      "398/398 [==============================] - 0s 45us/step - loss: 2.4794 - accuracy: 0.6709 - val_loss: 0.9307 - val_accuracy: 0.8363\n",
      "Epoch 5/100\n",
      "398/398 [==============================] - 0s 22us/step - loss: 1.2580 - accuracy: 0.7889 - val_loss: 0.4481 - val_accuracy: 0.9064\n",
      "Epoch 6/100\n",
      "398/398 [==============================] - 0s 28us/step - loss: 1.1210 - accuracy: 0.8216 - val_loss: 3.2502 - val_accuracy: 0.7251\n",
      "Epoch 7/100\n",
      "398/398 [==============================] - 0s 31us/step - loss: 2.8446 - accuracy: 0.6809 - val_loss: 0.9868 - val_accuracy: 0.8538\n",
      "Epoch 8/100\n",
      "398/398 [==============================] - 0s 38us/step - loss: 1.0309 - accuracy: 0.8291 - val_loss: 1.4880 - val_accuracy: 0.8187\n",
      "Epoch 9/100\n",
      "398/398 [==============================] - 0s 45us/step - loss: 0.7707 - accuracy: 0.8744 - val_loss: 0.3759 - val_accuracy: 0.9181\n",
      "Epoch 10/100\n",
      "398/398 [==============================] - 0s 80us/step - loss: 0.9912 - accuracy: 0.8065 - val_loss: 6.1694 - val_accuracy: 0.3977\n",
      "Epoch 11/100\n",
      "398/398 [==============================] - 0s 37us/step - loss: 2.5178 - accuracy: 0.7286 - val_loss: 2.8365 - val_accuracy: 0.7602\n",
      "Epoch 12/100\n",
      "398/398 [==============================] - 0s 47us/step - loss: 1.9779 - accuracy: 0.7889 - val_loss: 0.5596 - val_accuracy: 0.9240\n",
      "Epoch 13/100\n",
      "398/398 [==============================] - 0s 31us/step - loss: 1.1466 - accuracy: 0.8442 - val_loss: 0.9714 - val_accuracy: 0.8889\n",
      "Epoch 14/100\n",
      "398/398 [==============================] - 0s 48us/step - loss: 1.7754 - accuracy: 0.7437 - val_loss: 0.9300 - val_accuracy: 0.7953\n",
      "Epoch 15/100\n",
      "398/398 [==============================] - 0s 38us/step - loss: 0.9713 - accuracy: 0.8744 - val_loss: 0.8892 - val_accuracy: 0.8012\n",
      "Epoch 16/100\n",
      "398/398 [==============================] - 0s 36us/step - loss: 1.2780 - accuracy: 0.8141 - val_loss: 0.4086 - val_accuracy: 0.8889\n",
      "Epoch 17/100\n",
      "398/398 [==============================] - 0s 34us/step - loss: 1.2866 - accuracy: 0.8266 - val_loss: 3.0659 - val_accuracy: 0.5088\n",
      "Epoch 18/100\n",
      "398/398 [==============================] - 0s 23us/step - loss: 1.8237 - accuracy: 0.7513 - val_loss: 0.6870 - val_accuracy: 0.8363\n",
      "Epoch 19/100\n",
      "398/398 [==============================] - 0s 36us/step - loss: 1.1803 - accuracy: 0.8291 - val_loss: 0.5101 - val_accuracy: 0.8596\n",
      "Epoch 20/100\n",
      "398/398 [==============================] - 0s 30us/step - loss: 0.5660 - accuracy: 0.8894 - val_loss: 3.9431 - val_accuracy: 0.4620\n",
      "Epoch 21/100\n",
      "398/398 [==============================] - 0s 19us/step - loss: 3.0384 - accuracy: 0.6432 - val_loss: 0.7865 - val_accuracy: 0.9181\n",
      "Epoch 22/100\n",
      "398/398 [==============================] - 0s 26us/step - loss: 0.6402 - accuracy: 0.8995 - val_loss: 0.5020 - val_accuracy: 0.8596\n",
      "Epoch 23/100\n",
      "398/398 [==============================] - 0s 22us/step - loss: 0.4759 - accuracy: 0.8894 - val_loss: 4.6949 - val_accuracy: 0.4327\n",
      "Epoch 24/100\n",
      "398/398 [==============================] - 0s 32us/step - loss: 2.2811 - accuracy: 0.6884 - val_loss: 0.7117 - val_accuracy: 0.9123\n",
      "Epoch 25/100\n",
      "398/398 [==============================] - 0s 18us/step - loss: 0.7271 - accuracy: 0.8945 - val_loss: 1.1628 - val_accuracy: 0.7661\n",
      "Epoch 26/100\n",
      "398/398 [==============================] - 0s 28us/step - loss: 2.5112 - accuracy: 0.6910 - val_loss: 0.6022 - val_accuracy: 0.8538\n",
      "Epoch 27/100\n",
      "398/398 [==============================] - 0s 21us/step - loss: 0.5438 - accuracy: 0.8995 - val_loss: 2.0839 - val_accuracy: 0.8304\n",
      "Epoch 28/100\n",
      "398/398 [==============================] - 0s 32us/step - loss: 1.0282 - accuracy: 0.8769 - val_loss: 0.3769 - val_accuracy: 0.9357\n",
      "Epoch 29/100\n",
      "398/398 [==============================] - 0s 23us/step - loss: 0.5174 - accuracy: 0.8970 - val_loss: 1.5040 - val_accuracy: 0.8421\n",
      "Epoch 30/100\n",
      "398/398 [==============================] - 0s 29us/step - loss: 2.2314 - accuracy: 0.7688 - val_loss: 1.1914 - val_accuracy: 0.8830\n",
      "Epoch 31/100\n",
      "398/398 [==============================] - 0s 41us/step - loss: 0.9328 - accuracy: 0.8693 - val_loss: 0.3927 - val_accuracy: 0.9123\n",
      "Epoch 32/100\n",
      "398/398 [==============================] - 0s 24us/step - loss: 0.4580 - accuracy: 0.9045 - val_loss: 0.7116 - val_accuracy: 0.8480\n",
      "Epoch 33/100\n",
      "398/398 [==============================] - 0s 29us/step - loss: 0.8790 - accuracy: 0.8744 - val_loss: 3.3043 - val_accuracy: 0.5205\n",
      "Epoch 34/100\n",
      "398/398 [==============================] - 0s 25us/step - loss: 4.0261 - accuracy: 0.5678 - val_loss: 0.6035 - val_accuracy: 0.8596\n",
      "Epoch 35/100\n",
      "398/398 [==============================] - 0s 30us/step - loss: 0.5062 - accuracy: 0.8945 - val_loss: 0.4129 - val_accuracy: 0.9357\n",
      "Epoch 36/100\n",
      "398/398 [==============================] - 0s 24us/step - loss: 0.4499 - accuracy: 0.8945 - val_loss: 0.3697 - val_accuracy: 0.9357\n",
      "Epoch 37/100\n",
      "398/398 [==============================] - 0s 29us/step - loss: 0.5273 - accuracy: 0.8693 - val_loss: 1.9378 - val_accuracy: 0.8363\n",
      "Epoch 38/100\n",
      "398/398 [==============================] - 0s 24us/step - loss: 1.4730 - accuracy: 0.8166 - val_loss: 1.2690 - val_accuracy: 0.8772\n",
      "Epoch 39/100\n",
      "398/398 [==============================] - 0s 22us/step - loss: 0.7505 - accuracy: 0.8869 - val_loss: 1.2805 - val_accuracy: 0.7953\n",
      "Epoch 40/100\n",
      "398/398 [==============================] - 0s 23us/step - loss: 0.7944 - accuracy: 0.8618 - val_loss: 1.2074 - val_accuracy: 0.8772\n",
      "Epoch 41/100\n",
      "398/398 [==============================] - 0s 32us/step - loss: 1.0139 - accuracy: 0.8869 - val_loss: 1.6407 - val_accuracy: 0.7076\n",
      "Epoch 42/100\n",
      "398/398 [==============================] - 0s 29us/step - loss: 1.9478 - accuracy: 0.7487 - val_loss: 0.4918 - val_accuracy: 0.9357\n",
      "Epoch 43/100\n",
      "398/398 [==============================] - 0s 35us/step - loss: 0.4617 - accuracy: 0.9121 - val_loss: 0.4514 - val_accuracy: 0.8713\n",
      "Epoch 44/100\n",
      "398/398 [==============================] - 0s 20us/step - loss: 0.5900 - accuracy: 0.8719 - val_loss: 1.8708 - val_accuracy: 0.6140\n",
      "Epoch 45/100\n",
      "398/398 [==============================] - 0s 23us/step - loss: 2.4326 - accuracy: 0.6985 - val_loss: 0.7345 - val_accuracy: 0.9123\n",
      "Epoch 46/100\n",
      "398/398 [==============================] - 0s 25us/step - loss: 0.5190 - accuracy: 0.9020 - val_loss: 0.7624 - val_accuracy: 0.9123\n",
      "Epoch 47/100\n",
      "398/398 [==============================] - 0s 30us/step - loss: 0.5609 - accuracy: 0.8869 - val_loss: 1.0028 - val_accuracy: 0.8889\n",
      "Epoch 48/100\n",
      "398/398 [==============================] - 0s 23us/step - loss: 0.8451 - accuracy: 0.8819 - val_loss: 0.3946 - val_accuracy: 0.9415\n",
      "Epoch 49/100\n",
      "398/398 [==============================] - 0s 19us/step - loss: 0.8937 - accuracy: 0.8794 - val_loss: 0.3904 - val_accuracy: 0.8889\n",
      "Epoch 50/100\n",
      "398/398 [==============================] - 0s 29us/step - loss: 0.4379 - accuracy: 0.9045 - val_loss: 1.5594 - val_accuracy: 0.7193\n",
      "Epoch 51/100\n",
      "398/398 [==============================] - 0s 22us/step - loss: 1.9984 - accuracy: 0.7688 - val_loss: 1.0111 - val_accuracy: 0.8012\n",
      "Epoch 52/100\n",
      "398/398 [==============================] - 0s 23us/step - loss: 0.7073 - accuracy: 0.8920 - val_loss: 4.5330 - val_accuracy: 0.4269\n",
      "Epoch 53/100\n",
      "398/398 [==============================] - 0s 25us/step - loss: 2.3583 - accuracy: 0.7236 - val_loss: 0.3865 - val_accuracy: 0.9357\n",
      "Epoch 54/100\n",
      "398/398 [==============================] - 0s 25us/step - loss: 0.4391 - accuracy: 0.9196 - val_loss: 0.4023 - val_accuracy: 0.9474\n",
      "Epoch 55/100\n",
      "398/398 [==============================] - 0s 29us/step - loss: 0.6259 - accuracy: 0.8869 - val_loss: 1.8613 - val_accuracy: 0.6257\n",
      "Epoch 56/100\n",
      "398/398 [==============================] - 0s 33us/step - loss: 1.8681 - accuracy: 0.7688 - val_loss: 2.1710 - val_accuracy: 0.6023\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 0s 23us/step - loss: 1.4087 - accuracy: 0.7814 - val_loss: 0.4277 - val_accuracy: 0.9415\n",
      "Epoch 58/100\n",
      "398/398 [==============================] - 0s 29us/step - loss: 0.3936 - accuracy: 0.9095 - val_loss: 0.8714 - val_accuracy: 0.8947\n",
      "Epoch 59/100\n",
      "398/398 [==============================] - 0s 32us/step - loss: 1.1949 - accuracy: 0.8417 - val_loss: 2.5512 - val_accuracy: 0.8187\n",
      "Epoch 60/100\n",
      "398/398 [==============================] - 0s 16us/step - loss: 0.8923 - accuracy: 0.8894 - val_loss: 0.8854 - val_accuracy: 0.8070\n",
      "Epoch 61/100\n",
      "398/398 [==============================] - 0s 28us/step - loss: 1.0411 - accuracy: 0.8492 - val_loss: 0.3109 - val_accuracy: 0.9298\n",
      "Epoch 62/100\n",
      "398/398 [==============================] - 0s 19us/step - loss: 0.4285 - accuracy: 0.9121 - val_loss: 1.7259 - val_accuracy: 0.6082\n",
      "Epoch 63/100\n",
      "398/398 [==============================] - 0s 20us/step - loss: 1.7154 - accuracy: 0.7563 - val_loss: 0.8075 - val_accuracy: 0.8421\n",
      "Epoch 64/100\n",
      "398/398 [==============================] - 0s 20us/step - loss: 0.6689 - accuracy: 0.8869 - val_loss: 3.3830 - val_accuracy: 0.4912\n",
      "Epoch 65/100\n",
      "398/398 [==============================] - 0s 20us/step - loss: 2.2610 - accuracy: 0.7563 - val_loss: 0.7110 - val_accuracy: 0.8596\n",
      "Epoch 66/100\n",
      "398/398 [==============================] - 0s 25us/step - loss: 0.4406 - accuracy: 0.8945 - val_loss: 0.4387 - val_accuracy: 0.8713\n",
      "Epoch 67/100\n",
      "398/398 [==============================] - 0s 17us/step - loss: 0.4281 - accuracy: 0.9196 - val_loss: 1.3455 - val_accuracy: 0.7076\n",
      "Epoch 68/100\n",
      "398/398 [==============================] - 0s 25us/step - loss: 1.7121 - accuracy: 0.7789 - val_loss: 0.5332 - val_accuracy: 0.8596\n",
      "Epoch 69/100\n",
      "398/398 [==============================] - 0s 16us/step - loss: 0.4597 - accuracy: 0.9045 - val_loss: 0.3272 - val_accuracy: 0.9357\n",
      "Epoch 70/100\n",
      "398/398 [==============================] - 0s 21us/step - loss: 0.4512 - accuracy: 0.9020 - val_loss: 3.6049 - val_accuracy: 0.7836\n",
      "Epoch 71/100\n",
      "398/398 [==============================] - 0s 25us/step - loss: 2.1116 - accuracy: 0.7714 - val_loss: 0.3858 - val_accuracy: 0.9415\n",
      "Epoch 72/100\n",
      "398/398 [==============================] - 0s 16us/step - loss: 0.3783 - accuracy: 0.9171 - val_loss: 0.5268 - val_accuracy: 0.8538\n",
      "Epoch 73/100\n",
      "398/398 [==============================] - 0s 27us/step - loss: 1.2099 - accuracy: 0.7764 - val_loss: 0.5906 - val_accuracy: 0.9123\n",
      "Epoch 74/100\n",
      "398/398 [==============================] - 0s 20us/step - loss: 0.4098 - accuracy: 0.9196 - val_loss: 0.7778 - val_accuracy: 0.8070\n",
      "Epoch 75/100\n",
      "398/398 [==============================] - 0s 21us/step - loss: 0.8353 - accuracy: 0.8769 - val_loss: 0.4146 - val_accuracy: 0.9298\n",
      "Epoch 76/100\n",
      "398/398 [==============================] - 0s 20us/step - loss: 0.4823 - accuracy: 0.9095 - val_loss: 0.3605 - val_accuracy: 0.9415\n",
      "Epoch 77/100\n",
      "398/398 [==============================] - 0s 18us/step - loss: 1.6743 - accuracy: 0.7990 - val_loss: 0.3496 - val_accuracy: 0.9474\n",
      "Epoch 78/100\n",
      "398/398 [==============================] - 0s 26us/step - loss: 0.4156 - accuracy: 0.9146 - val_loss: 4.4325 - val_accuracy: 0.4444\n",
      "Epoch 79/100\n",
      "398/398 [==============================] - 0s 15us/step - loss: 2.5077 - accuracy: 0.7211 - val_loss: 0.3417 - val_accuracy: 0.9298\n",
      "Epoch 80/100\n",
      "398/398 [==============================] - 0s 23us/step - loss: 0.4852 - accuracy: 0.9070 - val_loss: 0.6184 - val_accuracy: 0.8596\n",
      "Epoch 81/100\n",
      "398/398 [==============================] - 0s 20us/step - loss: 0.4685 - accuracy: 0.9095 - val_loss: 0.3364 - val_accuracy: 0.9123\n",
      "Epoch 82/100\n",
      "398/398 [==============================] - 0s 18us/step - loss: 0.4162 - accuracy: 0.9171 - val_loss: 1.3181 - val_accuracy: 0.8596\n",
      "Epoch 83/100\n",
      "398/398 [==============================] - 0s 23us/step - loss: 1.7447 - accuracy: 0.7965 - val_loss: 0.4056 - val_accuracy: 0.9474\n",
      "Epoch 84/100\n",
      "398/398 [==============================] - 0s 18us/step - loss: 0.4318 - accuracy: 0.9171 - val_loss: 1.7248 - val_accuracy: 0.6140\n",
      "Epoch 85/100\n",
      "398/398 [==============================] - 0s 27us/step - loss: 1.7560 - accuracy: 0.7688 - val_loss: 0.3666 - val_accuracy: 0.9474\n",
      "Epoch 86/100\n",
      "398/398 [==============================] - 0s 22us/step - loss: 0.4531 - accuracy: 0.9095 - val_loss: 0.5273 - val_accuracy: 0.8655\n",
      "Epoch 87/100\n",
      "398/398 [==============================] - 0s 18us/step - loss: 0.5208 - accuracy: 0.9070 - val_loss: 0.4538 - val_accuracy: 0.9298\n",
      "Epoch 88/100\n",
      "398/398 [==============================] - 0s 21us/step - loss: 1.7968 - accuracy: 0.7839 - val_loss: 0.4452 - val_accuracy: 0.8830\n",
      "Epoch 89/100\n",
      "398/398 [==============================] - 0s 20us/step - loss: 0.4017 - accuracy: 0.9121 - val_loss: 0.4412 - val_accuracy: 0.8830\n",
      "Epoch 90/100\n",
      "398/398 [==============================] - 0s 26us/step - loss: 0.4889 - accuracy: 0.9095 - val_loss: 6.0761 - val_accuracy: 0.3684\n",
      "Epoch 91/100\n",
      "398/398 [==============================] - 0s 18us/step - loss: 2.6753 - accuracy: 0.7136 - val_loss: 0.6027 - val_accuracy: 0.8538\n",
      "Epoch 92/100\n",
      "398/398 [==============================] - 0s 24us/step - loss: 0.6389 - accuracy: 0.8719 - val_loss: 2.9302 - val_accuracy: 0.5029\n",
      "Epoch 93/100\n",
      "398/398 [==============================] - 0s 22us/step - loss: 1.3548 - accuracy: 0.7513 - val_loss: 2.5931 - val_accuracy: 0.8187\n",
      "Epoch 94/100\n",
      "398/398 [==============================] - 0s 21us/step - loss: 0.9129 - accuracy: 0.8894 - val_loss: 0.3430 - val_accuracy: 0.9123\n",
      "Epoch 95/100\n",
      "398/398 [==============================] - 0s 28us/step - loss: 0.3696 - accuracy: 0.9095 - val_loss: 0.3417 - val_accuracy: 0.9532\n",
      "Epoch 96/100\n",
      "398/398 [==============================] - 0s 27us/step - loss: 0.4122 - accuracy: 0.9045 - val_loss: 1.1122 - val_accuracy: 0.8655\n",
      "Epoch 97/100\n",
      "398/398 [==============================] - 0s 17us/step - loss: 0.8641 - accuracy: 0.8744 - val_loss: 3.3515 - val_accuracy: 0.7895\n",
      "Epoch 98/100\n",
      "398/398 [==============================] - 0s 26us/step - loss: 1.3369 - accuracy: 0.8643 - val_loss: 0.4847 - val_accuracy: 0.8655\n",
      "Epoch 99/100\n",
      "398/398 [==============================] - 0s 23us/step - loss: 0.3727 - accuracy: 0.9196 - val_loss: 0.3045 - val_accuracy: 0.9474\n",
      "Epoch 100/100\n",
      "398/398 [==============================] - 0s 18us/step - loss: 0.4402 - accuracy: 0.9045 - val_loss: 3.0646 - val_accuracy: 0.8012\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# Start neural network\n",
    "number_of_features = X_train.shape[1]\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=50, activation='relu', input_shape=(number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=30, activation='relu'))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss='binary_crossentropy', # Cross-entropy\n",
    "                optimizer='rmsprop', # Root Mean Square Propagation\n",
    "                metrics=['accuracy']) # Accuracy performance metric\n",
    "\n",
    "# Train neural network\n",
    "history = network.fit(X_train, # Features\n",
    "                      y_train, # Target vector\n",
    "                      epochs=100, # Number of epochs\n",
    "                      batch_size=128, # Number of observations per batch\n",
    "                      validation_data=(X_test, y_test)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing set accuracy with logit is: 92.96%\n"
     ]
    }
   ],
   "source": [
    "ANNacc = \"92.96%\"\n",
    "print(f\"\\ntesting set accuracy with logit is: {ANNacc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. c) Compare the performance of the neural net with the logit. Which one seems better at the job and why might that be?\n",
    "Please also comment on which model you would choose to implement in a case like this and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We observe that performance is better with the logit.\n",
      "\n",
      "The neural network training with small datasets commonly shows worse performance than traditional machine learning methods\n",
      "\n",
      "A neural network model for binary classification can be worse than a logistic regression model because it is more difficult to train and it is more prone to overfitting than logistic regression.\n"
     ]
    }
   ],
   "source": [
    "print(\"We observe that performance is better with the logit.\")\n",
    "print()\n",
    "print(\"The neural network training with small datasets commonly shows worse performance than traditional machine learning methods\")\n",
    "print()\n",
    "print(\"A neural network model for binary classification can be worse than a logistic regression model because it is more difficult to train and it is more prone to overfitting than logistic regression.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:\n",
    "### Your coding skills have gained you a job as an options trader at a successful hedge fund! Congratulations!!\n",
    "\n",
    "At the first day, your boss comes to you and asks you, whether he should buy a _call option_* with a certain set of characteristics for 1â‚¬.\n",
    "\n",
    "    *A call option gives you the right (but not obligation) to buy a share for a certain strike price. In other words, if the stock price is higher than the strike price, you get the difference, otherwise, you get 0: \n",
    "    \n",
    "    callpayoff = max(stockprice - strikeprice, 0) \n",
    "\n",
    "To price the option, you shall build a monte-carlo simulator which generatates _1 000 000_ random walks, each representing the stock price in one year, which is when the option can be expired. By taking the average of these payouts, you will get the expected payout at expiry!\n",
    "\n",
    "Luckily, your boss has also given you the characteristics and hints for how the stock price moves: \n",
    "\n",
    "The stock price follows a student T distribution with 3 degrees of freedom (the rvs function within the t class from scipy.stats package is a great tool for creating random walks with this distribution: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html  ctrl+f \"rvs\") \n",
    "\n",
    "- The stock price today: 69\n",
    "- Stock returns follow student T distribution with 3 degrees of freedom\n",
    "- drift of 10% per year \n",
    "- volatility of 20%\n",
    "- strike price of the option is 96\n",
    "\n",
    "### Build a function that returns the price of the call option, and shows your boss what payout he can expect from the option in order to explain him, whether he should buy it or not.\n",
    "\n",
    "    P.S.: If you run into trouble on your way, you can always ask for tips from your boss and since he is a nice guy, he will also give a good grade for all attempts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  98.55640826,   82.39763039,  181.23834375, ...,   56.82259911,\n",
       "         59.95994789, -112.32316206])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "size = 1000000\n",
    "df = 3\n",
    "loc = 69\n",
    "scale = 20\n",
    "\n",
    "r = t.rvs(df, loc=loc, scale=scale, size=size)\n",
    "\n",
    "#drift of 10%\n",
    "result = r*1.1 \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.91339210157912"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On average the stock price is expected to rise from 69 to 75.9\n"
     ]
    }
   ],
   "source": [
    "np.average(result) \n",
    "print(\"On average the stock price is expected to rise from 69 to 75.9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.556408259347563,\n",
       " 0,\n",
       " 85.23834374535306,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 20.669870483113584,\n",
       " 0,\n",
       " 0,\n",
       " 0.8876890563362281,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 11.57834093185582,\n",
       " 0,\n",
       " 2.107034704303345,\n",
       " 0,\n",
       " 14.2891145901892,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1.9768402608346776,\n",
       " 0,\n",
       " 11.122140200688008,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 24.411123905681862,\n",
       " 0,\n",
       " 101.27450987045086,\n",
       " 0,\n",
       " 155.30257994794312,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8.55319132571536,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 24.45242941517165,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 14.162951709268398,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 50.513425378390394,\n",
       " 76.04387095597463,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 11.64155814491781,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 33.40499695208294,\n",
       " 9.685382525707496,\n",
       " 0,\n",
       " 0,\n",
       " 34.048742805295774,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3.149235531405438,\n",
       " 0,\n",
       " 19.847887117864744,\n",
       " 10.691044874827142,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 22.125026969338975,\n",
       " 0,\n",
       " 5.935588693107277,\n",
       " 10.950747504456828,\n",
       " 4.8003120788673215,\n",
       " 0,\n",
       " 0,\n",
       " 5.323927138889843,\n",
       " 0,\n",
       " 0,\n",
       " 8.601825469443398,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 25.573181582759233,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 36.44433974245101,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 36.145668734868394,\n",
       " 37.54622666677338,\n",
       " 0,\n",
       " 54.20878230860009,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 66.0414241333985,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 14.217289797565115,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.7176501657524028,\n",
       " 0,\n",
       " 0,\n",
       " 13.453460778960974,\n",
       " 0,\n",
       " 0,\n",
       " 39.37801988348468,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 53.52664491516967,\n",
       " 14.320362594333815,\n",
       " 0,\n",
       " 0,\n",
       " 26.83490513712401,\n",
       " 85.91649049225734,\n",
       " 0,\n",
       " 0,\n",
       " 3.161528013314438,\n",
       " 0,\n",
       " 0,\n",
       " 2.406542117362079,\n",
       " 0,\n",
       " 0,\n",
       " 74.13540937657316,\n",
       " 0,\n",
       " 59.20461829287089,\n",
       " 0,\n",
       " 0,\n",
       " 2.8536528394956235,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 35.09799789210089,\n",
       " 0,\n",
       " 13.413428968385162,\n",
       " 108.02411359067028,\n",
       " 2.5708260004805084,\n",
       " 0.4756021697056525,\n",
       " 0,\n",
       " 0,\n",
       " 16.38528999208023,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 9.98876036047291,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1.1482853771212973,\n",
       " 13.397374473871963,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 62.44133454963341,\n",
       " 0,\n",
       " 1.6916363861511883,\n",
       " 43.120569356188554,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 32.36972283311036,\n",
       " 0,\n",
       " 14.766972087269238,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 48.86355625715069,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 56.861727262991366,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 36.00990083871059,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 71.64179165284034,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2.046425674701041,\n",
       " 0,\n",
       " 10.905702759589815,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.649810075562641,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 9.650637162075043,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 12.360893604358893,\n",
       " 13.218367341163528,\n",
       " 10.761085840011148,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 30.4700438319078,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 20.110899220301192,\n",
       " 0.7801813842350782,\n",
       " 0,\n",
       " 11.809179622866978,\n",
       " 0,\n",
       " 18.34319133350526,\n",
       " 0,\n",
       " 57.981982973885124,\n",
       " 0,\n",
       " 3.6180736634713355,\n",
       " 0,\n",
       " 9.407664735690275,\n",
       " 0,\n",
       " 23.768891899159627,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3.6923986586416646,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6.448191231964444,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 50.89928007445175,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4.780046945914975,\n",
       " 0,\n",
       " 24.626475639999185,\n",
       " 0,\n",
       " 0,\n",
       " 5.595583320635569,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 18.533025029921006,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 14.069016865075298,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 86.68862771556186,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 84.68295798149134,\n",
       " 0,\n",
       " 20.543496631167983,\n",
       " 2.385323983766625,\n",
       " 0,\n",
       " 7.443330189254041,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8.442247779193053,\n",
       " 0,\n",
       " 12.738717462141224,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 32.970757632952115,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2.5350285521460023,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.13424773932662504,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 23.79559147594331,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2.709149950705367,\n",
       " 119.71742658563761,\n",
       " 0,\n",
       " 41.429933465662316,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 22.708536428956236,\n",
       " 0,\n",
       " 0.8042523189215558,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 10.774903994132828,\n",
       " 0,\n",
       " 14.76431407147409,\n",
       " 0,\n",
       " 21.7582150264454,\n",
       " 0,\n",
       " 12.089464403106277,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 164.10797897038685,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 53.92238600500917,\n",
       " 0,\n",
       " 0.3438826561197317,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3.5183582393601682,\n",
       " 41.51648303854344,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 18.828477292609904,\n",
       " 0,\n",
       " 4.1588399559584985,\n",
       " 26.896959787097288,\n",
       " 0,\n",
       " 1.8263629846012606,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 13.66727084384992,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 35.78956778305238,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 20.867013540195117,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 25.948831258998354,\n",
       " 0,\n",
       " 8.299000895654828,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 17.926611380374894,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 24.200410956636915,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 70.8260519686354,\n",
       " 0,\n",
       " 0,\n",
       " 53.896344070694454,\n",
       " 4.713534805343045,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8.633576988354932,\n",
       " 8.823035720249365,\n",
       " 4.47055552641433,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4.27579026868159,\n",
       " 0,\n",
       " 2.679030702198503,\n",
       " 0,\n",
       " 26.570995841062953,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 47.24393456908879,\n",
       " 0,\n",
       " 6.324387438022029,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 39.90822970448133,\n",
       " 0,\n",
       " 41.810835504900865,\n",
       " 2.0228074212251954,\n",
       " 0,\n",
       " 0,\n",
       " 10.750744941686662,\n",
       " 0,\n",
       " 0.8845375675889642,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 25.487613855238507,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1.2203344632587516,\n",
       " 0,\n",
       " 0,\n",
       " 10.244188774892663,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4.067685476059509,\n",
       " 0,\n",
       " 21.01272293518022,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 9.5788945395213,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 16.17115897675758,\n",
       " 0,\n",
       " 27.97685330410647,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 10.169198152516202,\n",
       " 0,\n",
       " 0,\n",
       " 2.0881824399320266,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 52.96791426800192,\n",
       " 5.815866161769449,\n",
       " 0,\n",
       " 76.57336932390652,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5.530455399380742,\n",
       " 7.639480652537657,\n",
       " 0,\n",
       " 0,\n",
       " 140.26234129546606,\n",
       " 0,\n",
       " 0,\n",
       " 5.10971999190798,\n",
       " 69.19316225814111,\n",
       " 0,\n",
       " 23.582163743182278,\n",
       " 0,\n",
       " 0,\n",
       " 145.81485974286062,\n",
       " 0,\n",
       " 31.30721622694709,\n",
       " 7.777390558214208,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7.732728275942989,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 24.198983926700876,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.6075495295919922,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 24.687564903051225,\n",
       " 15.037665662659663,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 41.99608689106424,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4.426822167748583,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 12.253421428253816,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 14.612790198053247,\n",
       " 0,\n",
       " 42.54232310271473,\n",
       " 0,\n",
       " 0,\n",
       " 20.29458737915843,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 17.359658295938388,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1.6096268611284614,\n",
       " 0,\n",
       " 0,\n",
       " 17.197443973723352,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 34.31607990636968,\n",
       " 0,\n",
       " 6.563107433058292,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 22.322991385208397,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 13.36594419653791,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 24.360473549459442,\n",
       " 11.17903584405795,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 23.2535219921417,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 9.294045616965988,\n",
       " 0,\n",
       " 0,\n",
       " 6.741937300914714,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1.9985878898777685,\n",
       " 0,\n",
       " 0.6798653798010292,\n",
       " 0,\n",
       " 4.785071899209527,\n",
       " 14.226496250743097,\n",
       " 3.1419443403830343,\n",
       " 7.554602179471502,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 40.46806420324651,\n",
       " 1.5652264837917897,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1.2462873767718605,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3.3023071121111656,\n",
       " 0,\n",
       " 85.270766606277,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7.4376069939178535,\n",
       " 0,\n",
       " 42.91183553073577,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 26.592100419760712,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 20.269955230466792,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 66.2212281538389,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 87.89088975693832,\n",
       " 17.399146650695286,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's calculate the callpayoff = max(stockprice - strikeprice, 0) \n",
    "strikeprice = 96\n",
    "\n",
    "payoff = []\n",
    "\n",
    "for stockprice in result:\n",
    "    callpayoff = max(stockprice - strikeprice, 0)\n",
    "    payoff.append(callpayoff)\n",
    "    \n",
    "payoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5564082593475632,\n",
       " -1,\n",
       " 84.23834374535306,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 19.669870483113584,\n",
       " -1,\n",
       " -1,\n",
       " -0.11231094366377192,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 10.57834093185582,\n",
       " -1,\n",
       " 1.107034704303345,\n",
       " -1,\n",
       " 13.2891145901892,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 0.9768402608346776,\n",
       " -1,\n",
       " 10.122140200688008,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 23.411123905681862,\n",
       " -1,\n",
       " 100.27450987045086,\n",
       " -1,\n",
       " 154.30257994794312,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 7.5531913257153604,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 23.45242941517165,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 13.162951709268398,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 49.513425378390394,\n",
       " 75.04387095597463,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 10.64155814491781,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 32.40499695208294,\n",
       " 8.685382525707496,\n",
       " -1,\n",
       " -1,\n",
       " 33.048742805295774,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 2.149235531405438,\n",
       " -1,\n",
       " 18.847887117864744,\n",
       " 9.691044874827142,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 21.125026969338975,\n",
       " -1,\n",
       " 4.935588693107277,\n",
       " 9.950747504456828,\n",
       " 3.8003120788673215,\n",
       " -1,\n",
       " -1,\n",
       " 4.323927138889843,\n",
       " -1,\n",
       " -1,\n",
       " 7.601825469443398,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 24.573181582759233,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 35.44433974245101,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 35.145668734868394,\n",
       " 36.54622666677338,\n",
       " -1,\n",
       " 53.20878230860009,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 65.0414241333985,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 13.217289797565115,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -0.2823498342475972,\n",
       " -1,\n",
       " -1,\n",
       " 12.453460778960974,\n",
       " -1,\n",
       " -1,\n",
       " 38.37801988348468,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 52.52664491516967,\n",
       " 13.320362594333815,\n",
       " -1,\n",
       " -1,\n",
       " 25.83490513712401,\n",
       " 84.91649049225734,\n",
       " -1,\n",
       " -1,\n",
       " 2.161528013314438,\n",
       " -1,\n",
       " -1,\n",
       " 1.4065421173620791,\n",
       " -1,\n",
       " -1,\n",
       " 73.13540937657316,\n",
       " -1,\n",
       " 58.20461829287089,\n",
       " -1,\n",
       " -1,\n",
       " 1.8536528394956235,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 34.09799789210089,\n",
       " -1,\n",
       " 12.413428968385162,\n",
       " 107.02411359067028,\n",
       " 1.5708260004805084,\n",
       " -0.5243978302943475,\n",
       " -1,\n",
       " -1,\n",
       " 15.385289992080232,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 8.98876036047291,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 0.1482853771212973,\n",
       " 12.397374473871963,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 61.44133454963341,\n",
       " -1,\n",
       " 0.6916363861511883,\n",
       " 42.120569356188554,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 31.36972283311036,\n",
       " -1,\n",
       " 13.766972087269238,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 47.86355625715069,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 55.861727262991366,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 35.00990083871059,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 70.64179165284034,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1.046425674701041,\n",
       " -1,\n",
       " 9.905702759589815,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -0.35018992443735897,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 8.650637162075043,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 11.360893604358893,\n",
       " 12.218367341163528,\n",
       " 9.761085840011148,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 29.4700438319078,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 19.110899220301192,\n",
       " -0.2198186157649218,\n",
       " -1,\n",
       " 10.809179622866978,\n",
       " -1,\n",
       " 17.34319133350526,\n",
       " -1,\n",
       " 56.981982973885124,\n",
       " -1,\n",
       " 2.6180736634713355,\n",
       " -1,\n",
       " 8.407664735690275,\n",
       " -1,\n",
       " 22.768891899159627,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 2.6923986586416646,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 5.448191231964444,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 49.89928007445175,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 3.7800469459149753,\n",
       " -1,\n",
       " 23.626475639999185,\n",
       " -1,\n",
       " -1,\n",
       " 4.595583320635569,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 17.533025029921006,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 13.069016865075298,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 85.68862771556186,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 83.68295798149134,\n",
       " -1,\n",
       " 19.543496631167983,\n",
       " 1.3853239837666251,\n",
       " -1,\n",
       " 6.443330189254041,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 7.442247779193053,\n",
       " -1,\n",
       " 11.738717462141224,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 31.970757632952115,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1.5350285521460023,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -0.865752260673375,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 22.79559147594331,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1.709149950705367,\n",
       " 118.71742658563761,\n",
       " -1,\n",
       " 40.429933465662316,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 21.708536428956236,\n",
       " -1,\n",
       " -0.1957476810784442,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 9.774903994132828,\n",
       " -1,\n",
       " 13.76431407147409,\n",
       " -1,\n",
       " 20.7582150264454,\n",
       " -1,\n",
       " 11.089464403106277,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 163.10797897038685,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 52.92238600500917,\n",
       " -1,\n",
       " -0.6561173438802683,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 2.5183582393601682,\n",
       " 40.51648303854344,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 17.828477292609904,\n",
       " -1,\n",
       " 3.1588399559584985,\n",
       " 25.896959787097288,\n",
       " -1,\n",
       " 0.8263629846012606,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 12.66727084384992,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 34.78956778305238,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 19.867013540195117,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 24.948831258998354,\n",
       " -1,\n",
       " 7.299000895654828,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 16.926611380374894,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 23.200410956636915,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 69.8260519686354,\n",
       " -1,\n",
       " -1,\n",
       " 52.896344070694454,\n",
       " 3.713534805343045,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 7.633576988354932,\n",
       " 7.823035720249365,\n",
       " 3.47055552641433,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 3.2757902686815896,\n",
       " -1,\n",
       " 1.679030702198503,\n",
       " -1,\n",
       " 25.570995841062953,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 46.24393456908879,\n",
       " -1,\n",
       " 5.324387438022029,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 38.90822970448133,\n",
       " -1,\n",
       " 40.810835504900865,\n",
       " 1.0228074212251954,\n",
       " -1,\n",
       " -1,\n",
       " 9.750744941686662,\n",
       " -1,\n",
       " -0.11546243241103582,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 24.487613855238507,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 0.2203344632587516,\n",
       " -1,\n",
       " -1,\n",
       " 9.244188774892663,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 3.067685476059509,\n",
       " -1,\n",
       " 20.01272293518022,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 8.5788945395213,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15.171158976757582,\n",
       " -1,\n",
       " 26.97685330410647,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 9.169198152516202,\n",
       " -1,\n",
       " -1,\n",
       " 1.0881824399320266,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 51.96791426800192,\n",
       " 4.815866161769449,\n",
       " -1,\n",
       " 75.57336932390652,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 4.530455399380742,\n",
       " 6.639480652537657,\n",
       " -1,\n",
       " -1,\n",
       " 139.26234129546606,\n",
       " -1,\n",
       " -1,\n",
       " 4.10971999190798,\n",
       " 68.19316225814111,\n",
       " -1,\n",
       " 22.582163743182278,\n",
       " -1,\n",
       " -1,\n",
       " 144.81485974286062,\n",
       " -1,\n",
       " 30.30721622694709,\n",
       " 6.777390558214208,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 6.732728275942989,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 23.198983926700876,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -0.39245047040800785,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 23.687564903051225,\n",
       " 14.037665662659663,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 40.99608689106424,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 3.4268221677485826,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 11.253421428253816,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 13.612790198053247,\n",
       " -1,\n",
       " 41.54232310271473,\n",
       " -1,\n",
       " -1,\n",
       " 19.29458737915843,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 16.359658295938388,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 0.6096268611284614,\n",
       " -1,\n",
       " -1,\n",
       " 16.197443973723352,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 33.31607990636968,\n",
       " -1,\n",
       " 5.563107433058292,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 21.322991385208397,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 12.36594419653791,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 23.360473549459442,\n",
       " 10.17903584405795,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 22.2535219921417,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 8.294045616965988,\n",
       " -1,\n",
       " -1,\n",
       " 5.741937300914714,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 0.9985878898777685,\n",
       " -1,\n",
       " -0.32013462019897077,\n",
       " -1,\n",
       " 3.7850718992095267,\n",
       " 13.226496250743097,\n",
       " 2.1419443403830343,\n",
       " 6.554602179471502,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 39.46806420324651,\n",
       " 0.5652264837917897,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 0.24628737677186052,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 2.3023071121111656,\n",
       " -1,\n",
       " 84.270766606277,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 6.4376069939178535,\n",
       " -1,\n",
       " 41.91183553073577,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 25.592100419760712,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 19.269955230466792,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 65.2212281538389,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 86.89088975693832,\n",
       " 16.399146650695286,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " ...]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We include the 1â‚¬ cost of the call option\n",
    "\n",
    "final_result = []\n",
    "for e in payoff:\n",
    "    aux = e - 1 \n",
    "    final_result.append(aux)\n",
    "    \n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1984496073056645"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average return of the call option that we can expect is 4.20â‚¬. So we recommend our boss to buy it.\n"
     ]
    }
   ],
   "source": [
    "np.average(final_result)\n",
    "\n",
    "print(\"The average return of the call option that we can expect is 4.20â‚¬. So we recommend our boss to buy it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good luck!\n",
    "\n",
    "Don't forget: google, github and stack overflow are your best friends!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
